{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Definition of helper functions\n",
    "\n",
    "### makes soup\n",
    "def make_soup(url):\n",
    "    #print(\"---PAGE INFO---\")\n",
    "    response = requests.get(url)\n",
    "    print(\"status: \", response.status_code, url)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    return soup\n",
    "\n",
    "### removes filename if already exists\n",
    "import os\n",
    "def remove_if_exists(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "### writes to txt\n",
    "# input: <list> of dict entries, filename\n",
    "# output: appending txt file\n",
    "def append_file(dataList, outputFilename):\n",
    "    with open(outputFilename, \"a\") as outFile:\n",
    "        for dataEntry in dataList:\n",
    "            for key,value in dataEntry.items():\n",
    "                outFile.write(str(value) + \"\\t\")\n",
    "            outFile.write(\"\\n\")\n",
    "    return None\n",
    "\n",
    "### prints PARENT_URL summary stats\n",
    "# input: PARENT_URL\n",
    "# returns: total number of pages that all adds are displayed in\n",
    "import math\n",
    "def get_counts_cvonlinelt(PARENT_URL):\n",
    "#     print(\"---INITIAL INFO---\")\n",
    "    response = requests.get(PARENT_URL)\n",
    "#     print(\"status: \", response.status_code, PARENT_URL)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    extracted_numbers = soup.find(\"h1\").get_text()\n",
    "    extracted_numbers = re.findall(r'\\d+', extracted_numbers)\n",
    "    adCountForeign = 0\n",
    "    for item in extracted_numbers[1:]:\n",
    "        adCountForeign += int(item)\n",
    "    adCount = int(extracted_numbers[0]) - adCountForeign\n",
    "    webpageCount = math.ceil(int(adCount)/50)\n",
    "    print(\"Ads found:\", adCount, \"displayed in\", webpageCount, \"pages. Url:\", PARENT_URL)\n",
    "    return {'webpageCount': webpageCount, \n",
    "            'adCount': adCount}\n",
    "\n",
    "\n",
    "### gets total number of pages containing adds\n",
    "# input: url\n",
    "# returns: int\n",
    "def get_counts_cvlt(PARENT_URL):\n",
    "#     print(\"---INITIAL INFO---\")\n",
    "    response = requests.get(PARENT_URL)\n",
    "#     print(\"status: \", response.status_code, PARENT_URL)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    webpageCount = int(soup.find(\"span\", class_='paging-top').get_text().split()[2])\n",
    "    adCount = int(soup.find(\"span\", class_='lgray2').get_text().split()[1].replace('.', ''))\n",
    "    print(\"Ads found:\", adCount, \"displayed in\", webpageCount, \"pages. Url:\", PARENT_URL)\n",
    "    return {'webpageCount': webpageCount, \n",
    "            'adCount': adCount}\n",
    "\n",
    "\n",
    "### appends filename with '_month_day' before file extension\n",
    "# input: original filename\n",
    "# returns: (m, d) <tuple>\n",
    "import datetime\n",
    "def add_month_and_day_extension(filename):\n",
    "    month = datetime.date.today().month\n",
    "    day = datetime.date.today().day\n",
    "    return filename[:-4] + '_' + str(month) + '_' + str(day) + filename[-4:]\n",
    "\n",
    "### writes to txt\n",
    "# input: <str> text to write to log, <str> filename\n",
    "# output: appending txt file\n",
    "def scrape_log(text, outputFilename):\n",
    "    with open(outputFilename, \"a\") as outFile: \n",
    "        outFile.write(text)\n",
    "        outFile.write(\"\\n\")\n",
    "    return None\n",
    "\n",
    "\n",
    "##### Definition of scraping functions\n",
    "\n",
    "### scrapes job add info\n",
    "# CVONLINE.LT\n",
    "# input: url\n",
    "# output: list of dict entries (len=50)\n",
    "import requests, re, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "def scrape_cvonlinelt(url):\n",
    "    soup = make_soup(url)\n",
    "#     tr = soup.tbody.find_all('tr')\n",
    "    tr = soup.tbody.find_all('tr', itemtype='http://schema.org/JobPosting')\n",
    "    jobInfo50List = []\n",
    "        \n",
    "    for tag in tr:\n",
    "        # scrape job add info\n",
    "        jobTitle = tag.find(\"a\", class_=\"contentJobTitle\").get_text().strip()\n",
    "        jobUrl = \"http://\" + tag.find(\"a\", class_=\"contentJobTitle\").get(\"href\")[2:]\n",
    "        companyName = tag.find(\"a\", class_=\"contentCompanyName\").get_text().strip()\n",
    "        companyCity = tag.find(\"a\", class_=\"contentCompanyCity\").get_text().strip()\n",
    "\n",
    "        jobViewCount = tag.find(\"td\", itemprop=\"jobLocation\").get_text()\n",
    "        c = re.search(r'\\d+', jobViewCount)\n",
    "        try:\n",
    "            jobViewCount = c.group(0)\n",
    "        except:\n",
    "            jobViewCount = '0'\n",
    "\n",
    "        try:\n",
    "            jobDatesString = tag.find(\"td\", class_=\"t_jobs_tech alt\").p.next_sibling.get_text()\n",
    "        except:\n",
    "            try:\n",
    "                jobDatesString = tag.find(\"td\", class_=\"t_jobs_tech\").p.next_sibling.get_text()\n",
    "            except:\n",
    "                try:\n",
    "                    jobDatesString = tag.find(\"td\", class_=\"t_jobs_tech special\").p.next_sibling.get_text()\n",
    "                except:\n",
    "                    jobDatesString = '2099.01.01'\n",
    "        jobDates = re.findall(r'(\\d\\d\\d\\d.\\d\\d.\\d\\d)', jobDatesString)\n",
    "        jobDateDeadline = datetime.date(int(jobDates[0][0:4]), int(jobDates[0][5:7]), int(jobDates[0][8:10]))\n",
    "        jobDatePublished = datetime.date(int(jobDates[1][0:4]), int(jobDates[1][5:7]), int(jobDates[1][8:10]))\n",
    "        \n",
    "        jobData = {\n",
    "            \"companyName\" : companyName,\n",
    "            \"companyCity\" : companyCity,            \n",
    "            \"jobTitle\" : jobTitle, \n",
    "            \"jobViewCount\" : jobViewCount,\n",
    "            \"jobDateDeadline\" : jobDateDeadline,\n",
    "            \"jobDatePublished\" : jobDatePublished,\n",
    "            \"jobUrl\" : jobUrl,\n",
    "        }\n",
    "        jobInfo50List.append(jobData)\n",
    "        \n",
    "    return jobInfo50List\n",
    "\n",
    "### scrapes job add info\n",
    "# CV.LT\n",
    "# input: url\n",
    "# output: list of dict entries (len=200)\n",
    "import requests, re, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "def scrape_cvlt(url):\n",
    "    soup = make_soup(url)\n",
    "    tr = soup.tbody.find_all('tr', class_=['data sponsor', 'data'])\n",
    "    jobInfo200List = []\n",
    "    \n",
    "    for tag in tr:\n",
    "        # scrape job add info\n",
    "        jobTitle = tag.find(\"a\", itemprop=\"title\").get_text().strip()\n",
    "        jobUrl = \"http://www.cv.lt\" + tag.find(\"a\", itemprop=\"title\").get(\"href\")\n",
    "        companyName = tag.find(\"a\", itemprop=\"hiringOrganization\").get_text().strip()\n",
    "        companyCity = tag.find(\"a\", itemprop=\"\").get_text().strip()\n",
    "        try:\n",
    "            jobViewCount = tag.find(\"span\", class_=\"visited\").get_text().replace('.', '')\n",
    "        except:\n",
    "            jobViewCount = '0'\n",
    "\n",
    "        dateToday = datetime.date.today()\n",
    "        \n",
    "        # scrape timeSincePublished\n",
    "        # possible 4 basic cases: ['Vakar', 'Prieš X d.', 'Prieš X val.', 'Prieš X mėn.']\n",
    "        timeSincePublished = tag.td.div.get_text()\n",
    "        r1 = '^(Vakar)'\n",
    "        r2 = '^Prieš \\d+ \\w+.'\n",
    "        if re.findall(r1, timeSincePublished):\n",
    "            timeSincePublished = ''.join(re.findall(r1, timeSincePublished))\n",
    "        else:\n",
    "            timeSincePublished = ''.join(re.findall(r2, timeSincePublished))        \n",
    "        # calculate jobDatePublished\n",
    "        assert timeSincePublished[:5] == 'Prieš' or 'Vakar'\n",
    "        if timeSincePublished == 'Vakar':\n",
    "            jobDatePublished = dateToday + datetime.timedelta(days = -1)\n",
    "        else:\n",
    "            timeDeltaPublished = int(timeSincePublished.split()[1])\n",
    "            # timePeriod: possible 3 cases ['d.', 'l.', 'n.'] atitinkamai reiskia: dienas, valandas, menesius\n",
    "            timePeriod = timeSincePublished[-2:]\n",
    "            if timePeriod == 'd.':\n",
    "                jobDatePublished = dateToday + datetime.timedelta(days = -timeDeltaPublished)\n",
    "            elif timePeriod == 'l.':\n",
    "#                 jobDatePublished = dateToday + datetime.timedelta(hours = -timeDeltaPublished)\n",
    "                jobDatePublished = dateToday\n",
    "            elif timePeriod == 'n.':\n",
    "                jobDatePublished = dateToday + datetime.timedelta(days = -timeDeltaPublished*30)\n",
    "            else:\n",
    "                jobDatePublished = None\n",
    "                \n",
    "        # scrape timeBeforeDeadline\n",
    "        try:\n",
    "            timeBeforeDeadline = tag.find_all('td')[-2].get_text()\n",
    "        except:\n",
    "            timeBeforeDeadline = 'Liko -1 d.'\n",
    "        # calculate jobDateDeadline\n",
    "        try:\n",
    "            timeDeltaDeadline = int(timeBeforeDeadline.split()[1])\n",
    "        # case 'Liko 31+ d.'\n",
    "        except:\n",
    "            timeDeltaDeadline = 32\n",
    "        # calculate jobDateDeadline\n",
    "        jobDateDeadline = dateToday + datetime.timedelta(days = timeDeltaDeadline)        \n",
    "        \n",
    "        jobData = {\n",
    "            \"companyName\" : companyName,\n",
    "            \"companyCity\" : companyCity,            \n",
    "            \"jobTitle\" : jobTitle, \n",
    "            \"jobViewCount\" : jobViewCount,\n",
    "            \"jobDateDeadline\" : jobDateDeadline,\n",
    "            \"jobDatePublished\" : jobDatePublished,\n",
    "            \"jobUrl\" : jobUrl,\n",
    "        }\n",
    "        jobInfo200List.append(jobData)\n",
    "        \n",
    "    return jobInfo200List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTED @ Fri Sep 15 12:04:46 2017\n",
      "Ads found: 15253 displayed in 306 pages. Url: http://www.cvonline.lt/darbo-skelbimai/visi\n",
      "Ads found: 7729 displayed in 39 pages. Url: http://www.cv.lt/employee/announcementsAll.do?regular=true&ipp=200\n",
      "Total ads to scrape: 22982. Estimated time to complete: 0:25:30(h:m:s)\n",
      "Scraping...\n",
      "status:  200 http://www.cvonline.lt/darbo-skelbimai/visi?page=0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-12947daa99bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maddPagesCount_cvonlinelt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# gets job ads data as list of dict entries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0maddsData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_cvonlinelt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlConstruct_cvonlinelt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# writes to text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mappend_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddsData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-19f03a8a764b>\u001b[0m in \u001b[0;36mscrape_cvonlinelt\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m#     tr = soup.tbody.find_all('tr')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://schema.org/JobPosting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mjobInfo50List\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "##### Main Program - CVONLINE.LT and CV.LT\n",
    "\n",
    "import requests, re, os, time, datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "PARENT_URL_CVONLINELT = \"http://www.cvonline.lt/darbo-skelbimai/visi\"\n",
    "PARENT_URL_CVLT = 'http://www.cv.lt/employee/announcementsAll.do?regular=true&ipp=200'\n",
    "OUTPUT_FILENAME = \"job_ads.txt\"\n",
    "LOG_FILENAME = \"ads_log.txt\"\n",
    "sleep_time = 5 #seconds\n",
    "\n",
    "running_start_time = time.time() #for getting elapsed time later\n",
    "to_log = \"STARTED @ \" + time.ctime()\n",
    "scrape_log(to_log, LOG_FILENAME)\n",
    "print(to_log)\n",
    "\n",
    "OUTPUT_FILENAME = add_month_and_day_extension(OUTPUT_FILENAME)\n",
    "remove_if_exists(OUTPUT_FILENAME)\n",
    "\n",
    "countsDict_cvonlinelt = get_counts_cvonlinelt(PARENT_URL_CVONLINELT)\n",
    "countsDict_cvlt = get_counts_cvlt(PARENT_URL_CVLT)\n",
    "addPagesCount_cvonlinelt = countsDict_cvonlinelt['webpageCount']\n",
    "addPagesCount_cvlt = countsDict_cvlt['webpageCount']\n",
    "addPagesCount = max(addPagesCount_cvonlinelt, addPagesCount_cvlt)\n",
    "estimated_scrape_time = sleep_time*addPagesCount\n",
    "\n",
    "to_log = \"Total ads to scrape: \" + str(countsDict_cvonlinelt['adCount'] + countsDict_cvlt['adCount']) + \". Estimated time to complete: \" + str(datetime.timedelta(seconds=estimated_scrape_time)) + \"(h:m:s)\"\n",
    "scrape_log(to_log, LOG_FILENAME)\n",
    "print(to_log)\n",
    "\n",
    "addsDataComplete = []\n",
    "\n",
    "to_log = \"Scraping...\"\n",
    "scrape_log(to_log, LOG_FILENAME)\n",
    "print(to_log)\n",
    "\n",
    "for page in range(addPagesCount+1):\n",
    "    urlConstruct_cvonlinelt = PARENT_URL_CVONLINELT + \"?page=\" + str(page)\n",
    "    urlConstruct_cvlt = PARENT_URL_CVLT + '&page=' + str(page)\n",
    "    \n",
    "    # CVonline.lt\n",
    "    if page <= addPagesCount_cvonlinelt:\n",
    "        # gets job ads data as list of dict entries\n",
    "        addsData = scrape_cvonlinelt(urlConstruct_cvonlinelt)\n",
    "        # writes to text file\n",
    "        append_file(addsData, OUTPUT_FILENAME)\n",
    "        # saves to a list\n",
    "        for item in addsData:\n",
    "            addsDataComplete.append(item)\n",
    "    # CV.lt\n",
    "    if page <= addPagesCount_cvlt:\n",
    "        # gets job ads data as list of dict entries\n",
    "        addsData = scrape_cvlt(urlConstruct_cvlt)\n",
    "        # writes to text file\n",
    "        append_file(addsData, OUTPUT_FILENAME)\n",
    "        # saves to a list\n",
    "        for item in addsData:\n",
    "            addsDataComplete.append(item)\n",
    "            \n",
    "    # print status info\n",
    "    to_log = \"Page \" + str(page+1) + \" of \" + str(addPagesCount) + \" scraped, \" + \"total entries: \" + str(len(addsDataComplete))\n",
    "    scrape_log(to_log, LOG_FILENAME)\n",
    "    print(to_log)\n",
    "    time.sleep(sleep_time)\n",
    "#     break\n",
    "    \n",
    "running_finish_time = time.time() \n",
    "running_time = running_finish_time - running_start_time\n",
    "\n",
    "to_log = \"---SUMMARY---\"\n",
    "scrape_log(to_log, LOG_FILENAME)\n",
    "\n",
    "to_log = \"Completed in \" + str(datetime.timedelta(seconds=running_time))\n",
    "scrape_log(to_log, LOG_FILENAME)\n",
    "print(to_log)\n",
    "\n",
    "to_log = \"FINISHED @ \" + time.ctime()\n",
    "scrape_log(to_log, LOG_FILENAME)\n",
    "print(to_log)\n",
    "\n",
    "to_log = \"\\n\\n\"\n",
    "scrape_log(to_log, LOG_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
